{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc7248e",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nilearn import datasets\n",
    "from nilearn import plotting\n",
    "from nilearn.input_data import NiftiMapsMasker\n",
    "from nilearn.connectome import ConnectivityMeasure\n",
    "import nilearn.image as image\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import Sequential, Linear, ReLU, GRU, BatchNorm1d\n",
    "from torch_geometric.nn import EdgeConv, GCNConv, GraphConv, Node2Vec\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.convert_matrix import from_numpy_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb753af9",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd338478",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset_path = 'dataset'\n",
    "processed_path = 'processed'\n",
    "\n",
    "# Load dataset\n",
    "data = datasets.fetch_abide_pcp()\n",
    "\n",
    "# Load requisite atlases\n",
    "atlas_msdl = datasets.fetch_atlas_msdl()\n",
    "atlas_allen = datasets.fetch_atlas_allen_2011()\n",
    "atlas_difumo_64 = datasets.fetch_atlas_difumo(dimension=64, resolution_mm=2)\n",
    "atlas_difumo_128 = datasets.fetch_atlas_difumo(dimension=128, resolution_mm=2)\n",
    "\n",
    "atlas_1 = atlas_msdl\n",
    "atlas_1_name = 'msdl'\n",
    "\n",
    "atlas_2 = atlas_allen\n",
    "atlas_2_name = 'allen'\n",
    "\n",
    "# Define atlas paths\n",
    "corr_matrices_dirs = [f'{processed_path}/atlas_parcel/{atlas_1_name}/corr_matrices', f'{processed_path}/atlas_parcel/{atlas_2_name}/corr_matrices']\n",
    "pcorr_matrices_dirs = [f'{processed_path}/atlas_parcel/{atlas_1_name}/pcorr_matrices', f'{processed_path}/atlas_parcel/{atlas_2_name}/pcorr_matrices']\n",
    "phenotypic_file = f'{processed_path}/atlas_parcel/{atlas_1_name}/phenotypic_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1696e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(masker, n_subjects, data, atlas_dir):\n",
    "    \"\"\"\n",
    "    Extract ROI time series data from fMRI dataset.\n",
    "    \"\"\"\n",
    "    time_series_dir = f'{atlas_dir}/time_series'\n",
    "    phenotypic_file = f'{atlas_dir}/phenotypic_data.csv'\n",
    "\n",
    "    time_series = [0] * n_subjects\n",
    "    labels = [0] * n_subjects\n",
    "    age = [0] * n_subjects\n",
    "    sex = [0] * n_subjects\n",
    "\n",
    "    if not os.path.exists(time_series_dir):\n",
    "        print('Extracting time series.')\n",
    "        for i in range(0, n_subjects):\n",
    "            print('Subject:', i)\n",
    "            ts = masker.fit_transform(data.func_preproc[i])\n",
    "            time_series[i] = ts\n",
    "        for i in range(len(time_series)):\n",
    "            os.makedirs(time_series_dir, exist_ok=True)\n",
    "            np.savetxt(f'{time_series_dir}/time_series_{i:04d}.csv', time_series[i], delimiter=',')\n",
    "    else:\n",
    "        for i in range(0, n_subjects):\n",
    "            time_series[i] = np.loadtxt(f'{time_series_dir}/time_series_{i:04d}.csv', delimiter=',')\n",
    "    \n",
    "    if not os.path.exists(phenotypic_file):\n",
    "        for i in range(0, n_subjects):\n",
    "            labels[i] = data.phenotypic[i][7] - 1 # Whether or not the subject is autistic (0) or control (1)\n",
    "            age[i] = data.phenotypic[i][9] # age\n",
    "            sex[i] = data.phenotypic[i][10] - 1 # sex (0=male, 1=female)\n",
    "        phenotypic_data = np.array([labels, age, sex])\n",
    "        np.savetxt(phenotypic_file, phenotypic_data.T.astype(int), delimiter=',')\n",
    "    else:\n",
    "        phenotypic_data = np.loadtxt(phenotypic_file, delimiter=',')\n",
    "    \n",
    "    return time_series, phenotypic_data\n",
    "\n",
    "def cal_corr(time_series, atlas_dir):\n",
    "    \"\"\"\n",
    "    Calculate correlation and partial correlation matrices from time series data.\n",
    "    \"\"\"\n",
    "    \n",
    "    corr_matrices_dir = f'{atlas_dir}/corr_matrices'\n",
    "    pcorr_matrices_dir = f'{atlas_dir}/pcorr_matrices'\n",
    "    os.makedirs(corr_matrices_dir, exist_ok=True)\n",
    "    os.makedirs(pcorr_matrices_dir, exist_ok=True)\n",
    "\n",
    "    corr_measure = ConnectivityMeasure(kind='correlation')\n",
    "    pcorr_measure = ConnectivityMeasure(kind='partial correlation')\n",
    "\n",
    "    corr_matrices = corr_measure.fit_transform(time_series)\n",
    "    pcorr_matrices = pcorr_measure.fit_transform(time_series)\n",
    "\n",
    "    for i in range(0, len(corr_matrices)):\n",
    "        np.savetxt(f'{corr_matrices_dir}/corr_{i:04d}.csv', corr_matrices[i], delimiter=',')\n",
    "        np.savetxt(f'{pcorr_matrices_dir}/pcorr_{i:04d}.csv', pcorr_matrices[i], delimiter=',')\n",
    "    \n",
    "    return corr_matrices, pcorr_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dacfc991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set maskers from each atlas\n",
    "masker_1 = NiftiMapsMasker(maps_img=atlas_1.maps, standardize=True, memory='nilearn_cache')\n",
    "masker_2 = NiftiMapsMasker(maps_img=atlas_2.maps, standardize=True, memory='nilearn_cache')\n",
    "\n",
    "# Identify the number of subjects we wish to analyze\n",
    "n_subjects = int(len(data.func_preproc) * 1.0)\n",
    "\n",
    "# Load time series data\n",
    "time_series_1, phenotypic_data_1 = load_data(masker_1, n_subjects, data, f'{processed_path}/atlas_parcel/{atlas_1_name}')\n",
    "time_series_2, phenotypic_data_2 = load_data(masker_2, n_subjects, data, f'{processed_path}/atlas_parcel/{atlas_2_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "494e4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation and partial correlation matrices\n",
    "corr_matrices_1, pcorr_matrices_1 = cal_corr(time_series_1, f'{processed_path}/atlas_parcel/{atlas_1_name}')\n",
    "corr_matrices_2, pcorr_matrices_2 = cal_corr(time_series_2, f'{processed_path}/atlas_parcel/{atlas_2_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1c8e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot partial correlation matrix for one subject\n",
    "plotting.plot_matrix(corr_matrices_1[20], colorbar=True, vmax=1, vmin=-1)\n",
    "plotting.plot_matrix(corr_matrices_2[20], colorbar=True, vmax=1, vmin=-1)\n",
    "\n",
    "# Plot brain connectome for one subject\n",
    "atlas_coords = plotting.find_probabilistic_atlas_cut_coords(maps_img=atlas_1.maps)\n",
    "plotting.plot_connectome(pcorr_matrices_1[20], atlas_coords, edge_threshold=\"95%\")\n",
    "\n",
    "atlas_coords = plotting.find_probabilistic_atlas_cut_coords(maps_img=atlas_2.maps)\n",
    "plotting.plot_connectome(pcorr_matrices_2[20], atlas_coords, edge_threshold=\"95%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e5a276fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphPhenoDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, phenotypic_data=None, neighbor_ratio=0.1):\n",
    "        self.neighbor_ratio = neighbor_ratio\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\" Converts raw data into GNN-readable format by constructing\n",
    "        graphs out of connectivity matrices.\n",
    "        \"\"\"\n",
    "\n",
    "        # Paths of connectivity matrices\n",
    "        corr_path_list = sorted(os.listdir(corr_matrices_dirs[0]), key=lambda x: int(x[-8:-4]))\n",
    "        pcorr_path_list = sorted(os.listdir(pcorr_matrices_dirs[0]), key=lambda x: int(x[-8:-4]))\n",
    "\n",
    "        graphs = []\n",
    "        phenotypic_data = np.loadtxt(phenotypic_file, delimiter=',')\n",
    "        labels = torch.from_numpy(phenotypic_data[:, 0])\n",
    "        age = torch.from_numpy(phenotypic_data[:, 1])\n",
    "        sex = torch.from_numpy(phenotypic_data[:, 2])\n",
    "\n",
    "        for i in range(0, len(corr_path_list)):\n",
    "            corr_matrix_path = os.path.join(corr_matrices_dirs[0], corr_path_list[i])\n",
    "            pcorr_matrix_path = os.path.join(pcorr_matrices_dirs[0], pcorr_path_list[i])\n",
    "\n",
    "            # Load partial correlation matrices\n",
    "            pcorr_matrix_np = np.loadtxt(pcorr_matrix_path, delimiter=',')\n",
    "\n",
    "            index = np.abs(pcorr_matrix_np).argsort(axis=1)\n",
    "            n_rois = pcorr_matrix_np.shape[0]\n",
    "\n",
    "            # Take top k correlates (k = n_subjects * neighbor_ratio) as edges\n",
    "            for j in range(n_rois):\n",
    "                for k in range(n_rois - int(self.neighbor_ratio*n_rois)):\n",
    "                    pcorr_matrix_np[j, index[j, k]] = 0\n",
    "                #for k in range(n_rois - int(self.neighbor_ratio*n_rois), n_rois):\n",
    "                    #pcorr_matrix_np[j, index[j, k]] = 1\n",
    "\n",
    "            pcorr_matrix_nx = from_numpy_array(pcorr_matrix_np)\n",
    "            pcorr_matrix_data = from_networkx(pcorr_matrix_nx)\n",
    "\n",
    "            # Correlation matrix which will serve as our features\n",
    "            corr_matrix_np = np.loadtxt(corr_matrix_path, delimiter=',')\n",
    "\n",
    "            pcorr_matrix_data.x = torch.tensor(corr_matrix_np).float()\n",
    "            pcorr_matrix_data.y = labels[i].type(torch.LongTensor)\n",
    "            pcorr_matrix_data.age = age[i].type(torch.FloatTensor)\n",
    "            pcorr_matrix_data.sex = sex[i].type(torch.FloatTensor)\n",
    "\n",
    "            # Add to running list of all dataset items\n",
    "            graphs.append(pcorr_matrix_data)\n",
    "\n",
    "        data, slices = self.collate(graphs)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "class MultiGraphDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, neighbor_ratio=0.1):\n",
    "        self.neighbor_ratio = neighbor_ratio\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\" Converts raw data into GNN-readable format by constructing\n",
    "        graphs out of connectivity matrices.\n",
    "        \"\"\"\n",
    "\n",
    "        # Paths of connectivity matrices\n",
    "        corr_path_lists = []\n",
    "        pcorr_path_lists = []\n",
    "        \n",
    "        corr_path_lists.append(sorted(os.listdir(corr_matrices_dirs[0]), key=lambda x: int(x[-8:-4])))\n",
    "        corr_path_lists.append(sorted(os.listdir(corr_matrices_dirs[1]), key=lambda x: int(x[-8:-4])))\n",
    "\n",
    "        pcorr_path_lists.append(sorted(os.listdir(pcorr_matrices_dirs[0]), key=lambda x: int(x[-8:-4])))\n",
    "        pcorr_path_lists.append(sorted(os.listdir(pcorr_matrices_dirs[1]), key=lambda x: int(x[-8:-4])))\n",
    "\n",
    "        phenotypic_data = np.loadtxt(phenotypic_file, delimiter=',')\n",
    "        labels = torch.from_numpy(phenotypic_data[:, 0])\n",
    "        age = torch.from_numpy(phenotypic_data[:, 1])\n",
    "        sex = torch.from_numpy(phenotypic_data[:, 2])\n",
    "\n",
    "        data_list = []\n",
    "\n",
    "        for i in range(0, n_subjects):\n",
    "            graphs = []\n",
    "\n",
    "            for graph_i in range(2):\n",
    "                corr_matrix_path = os.path.join(corr_matrices_dirs[graph_i], corr_path_lists[graph_i][i])\n",
    "                pcorr_matrix_path = os.path.join(pcorr_matrices_dirs[graph_i], pcorr_path_lists[graph_i][i])\n",
    "\n",
    "                # Load partial correlation matrices\n",
    "                pcorr_matrix_np = np.loadtxt(pcorr_matrix_path, delimiter=',')\n",
    "\n",
    "                index = np.abs(pcorr_matrix_np).argsort(axis=1)\n",
    "                n_rois = pcorr_matrix_np.shape[0]\n",
    "\n",
    "                # Take top k correlates (k = n_subjects * neighbor_ratio) as edges\n",
    "                for j in range(n_rois):\n",
    "                    for k in range(n_rois - int(self.neighbor_ratio*n_rois)):\n",
    "                        pcorr_matrix_np[j, index[j, k]] = 0\n",
    "                    #for k in range(n_rois - int(self.neighbor_ratio*n_rois), n_rois):\n",
    "                        #pcorr_matrix_np[j, index[j, k]] = 1\n",
    "\n",
    "                pcorr_matrix_nx = from_numpy_array(pcorr_matrix_np)\n",
    "                pcorr_matrix_data = from_networkx(pcorr_matrix_nx)\n",
    "\n",
    "                # Correlation matrix which will serve as our features\n",
    "                corr_matrix_np = np.loadtxt(corr_matrix_path, delimiter=',')\n",
    "\n",
    "                pcorr_matrix_data.x = torch.tensor(corr_matrix_np).float()\n",
    "                pcorr_matrix_data.y = labels[i].type(torch.LongTensor)\n",
    "\n",
    "                # Add to running list of all dataset items\n",
    "                graphs.append(pcorr_matrix_data)\n",
    "        \n",
    "            data = Data(x1 = graphs[0].x, edge_index1 = graphs[0].edge_index,\n",
    "                        x2 = graphs[1].x, edge_index2 = graphs[1].edge_index,\n",
    "                        num_nodes1 = graphs[0].num_nodes, num_nodes2 = graphs[1].num_nodes, \n",
    "                        age = age[i].type(torch.FloatTensor), sex = sex[i].type(torch.FloatTensor),\n",
    "                        y = graphs[0].y)\n",
    "            data_list.append(data)\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84afa03",
   "metadata": {},
   "source": [
    "For the below, run one of the two cells to create either a multigraph dataset (MultiGraphDataset) or single graph dataset (GraphPhenoDataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for multigraph dataset\n",
    "\n",
    "dataset = MultiGraphDataset(f'multigraph_{atlas_1_name}_{atlas_2_name}')\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# Train/test split (80-20)\n",
    "train_share = int(len(dataset) * 0.8)\n",
    "\n",
    "train_dataset = dataset[:train_share]\n",
    "test_dataset = dataset[train_share:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e363a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this for single graph dataset\n",
    "\n",
    "dataset = GraphPhenoDataset(f'graph_{atlas_1_name}')\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# Train/test split (80-20)\n",
    "train_share = int(len(dataset) * 0.8)\n",
    "\n",
    "train_dataset = dataset[:train_share]\n",
    "test_dataset = dataset[train_share:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e79823a",
   "metadata": {},
   "source": [
    "## Graph Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1a59691d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNetwork_Pheno(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize MLPs used by EdgeConv layers\n",
    "        self.mlp1 = Sequential(Linear(2 * dataset.num_node_features, hidden_channels), ReLU())\n",
    "        self.mlp2 = Sequential(torch.nn.Linear(2 * hidden_channels, hidden_channels), ReLU())\n",
    "\n",
    "        # Initialize EdgeConv layers\n",
    "        self.conv1 = EdgeConv(self.mlp1, aggr='max')\n",
    "        self.conv2 = EdgeConv(self.mlp2, aggr='max')\n",
    "\n",
    "        self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(hidden_channels + 2, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\" Performs a forward pass on our simplified cGCN.\n",
    "\n",
    "        Parameters:\n",
    "        data (Data): Graph being passed into network.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor (N x 2): Probability distribution over class labels.\n",
    "        \"\"\"\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        age = data.age\n",
    "        sex = data.sex\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        x = torch.cat((x, age.unsqueeze(1), sex.unsqueeze(1)), 1)\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)        \n",
    "        x = self.lin2(x)\n",
    "\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GraphNetwork_Multi(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1_1 = GCNConv(dataset[0].x1.shape[0], hidden_channels)\n",
    "        self.conv2_1 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.bn1_1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2_1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "        self.conv1_2 = GCNConv(dataset[0].x2.shape[0], hidden_channels)\n",
    "        self.conv2_2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.bn1_2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "        self.bn2_2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "        self.lin1 = torch.nn.Linear(hidden_channels * 2 + 2, hidden_channels)\n",
    "        self.lin2 = torch.nn.Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        \"\"\" Performs a forward pass on our simplified cGCN.\n",
    "\n",
    "        Parameters:\n",
    "        data (Data): Graph being passed into network.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor (N x 2): Probability distribution over class labels.\n",
    "        \"\"\"\n",
    "        x1, x2 = data.x1, data.x2\n",
    "        edge_index1, edge_index2 = data.edge_index1, data.edge_index2\n",
    "        batch = data.batch\n",
    "        age = data.age\n",
    "        sex = data.sex\n",
    "\n",
    "        # Process graph 1\n",
    "        x1 = F.relu(self.bn1_1(self.conv1_1(x1, edge_index1)))\n",
    "        x1 = F.relu(self.bn2_1(self.conv2_1(x1, edge_index1)))\n",
    "        x1_batch, _ = torch.sort(torch.tile(batch, (int(len(x1)/len(batch)),)))\n",
    "        x1 = global_mean_pool(x1, x1_batch)  # Aggregate node features to graph features\n",
    "\n",
    "        # Process graph 2\n",
    "        x2 = F.relu(self.bn1_2(self.conv1_2(x2, edge_index2)))\n",
    "        x2 = F.relu(self.bn2_2(self.conv2_2(x2, edge_index2)))\n",
    "        x2_batch, _ = torch.sort(torch.tile(batch, (int(len(x2)/len(batch)),)))\n",
    "        x2 = global_mean_pool(x2, x2_batch)  # Aggregate node features to graph features\n",
    "\n",
    "        # Concatenate features from both graphs and classify\n",
    "        x = torch.cat((x1, x2, age.unsqueeze(1), sex.unsqueeze(1)), dim=1)\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b2d4c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, device, data_loader, optimizer):\n",
    "    \"\"\" \n",
    "    Performs an epoch of model training.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch)\n",
    "\n",
    "        loss = loss_fn(out, batch.y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def eval(model, device, loader):\n",
    "    \"\"\"\n",
    "    Calculate accuracy for all examples in a DataLoader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    cor = 0\n",
    "    tot = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = torch.argmax(model(batch), 1)\n",
    "\n",
    "        y = batch.y\n",
    "        cor += (pred == y).sum()\n",
    "        tot += pred.shape[0]\n",
    "\n",
    "    return cor, tot\n",
    "\n",
    "def eval_precision(model, device, loader):\n",
    "    \"\"\" \n",
    "    Calculate accuracy, precision, recall for all examples in a DataLoader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    TP, FP, TN, FN = 0.01, 0.01, 0.01, 0.01\n",
    "    \n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = torch.argmax(model(batch), 1)\n",
    "        \n",
    "        for pred_i in range(len(pred)):\n",
    "            if pred[pred_i]==0 and batch.y[pred_i]==0:\n",
    "                TP += 1\n",
    "            elif pred[pred_i]==0 and batch.y[pred_i]==1:\n",
    "                FP += 1\n",
    "            elif pred[pred_i]==1 and batch.y[pred_i]==1:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "\n",
    "    accuracy = (TP + TN) / (TP + FP + TN + FN)\n",
    "    precision = (TP) / (TP + FP)    \n",
    "    recall = (TP) / (TP + FN)\n",
    "\n",
    "    return accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d354b",
   "metadata": {},
   "source": [
    "### Multiscale GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c44db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GraphNetwork_Multi(32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(0, 75):\n",
    "    loss = train(model, loss_fn, device, train_loader, optimizer)\n",
    "    train_correct, train_samples = eval(model, device, train_loader)\n",
    "    test_correct, test_samples = eval(model, device, test_loader)\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_correct / train_samples:.2f}%, '\n",
    "          f'Test: {100 * test_correct / test_samples:.2f}%')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1dae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 75\n",
    "batch_size = 32\n",
    "n_splits = 5\n",
    "\n",
    "kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "fold_accuracy = []\n",
    "fold_precision = []\n",
    "fold_recall = []\n",
    "fold_time = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f\"Training fold {fold+1}/{n_splits}\")\n",
    "\n",
    "    # Create data subsets for the current fold\n",
    "    train_subset = torch.utils.data.Subset(dataset, train_idx)\n",
    "    val_subset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Initialize the model\n",
    "    model = GraphNetwork_Multi(32).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = train(model, loss_fn, device, train_loader, optimizer)\n",
    "        tot_correct, tot_samples = eval(model, device, val_loader)\n",
    "        accuracy, precision, recall = eval_precision(model, device, val_loader)\n",
    "        print(f\"Epoch {epoch+1}: Fold {fold+1} Validation Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    fold_accuracy.append(accuracy)\n",
    "    fold_precision.append(precision)\n",
    "    fold_recall.append(recall)\n",
    "    fold_time.append(end_time - start_time)\n",
    "\n",
    "average_accuracy = np.mean(fold_accuracy)\n",
    "print(f\"Average Cross-Validation Accuracy: {average_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c446fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_list = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "sparsity_accuracy = []\n",
    "sparsity_precision = []\n",
    "sparsity_recall = []\n",
    "sparsity_time = []\n",
    "\n",
    "for sparsity_i, sparsity in enumerate(sparsity_list):    \n",
    "    dataset_sparsity = MultiGraphDataset(f'multigraph_sparsity_test', neighbor_ratio=sparsity)\n",
    "\n",
    "    n_epochs = 75\n",
    "    batch_size = 32\n",
    "    n_splits = 5\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    fold_accuracy = []\n",
    "    fold_precision = []\n",
    "    fold_recall = []\n",
    "    fold_time = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset_sparsity)):\n",
    "        print(f\"Training fold {fold+1}/{n_splits}\")\n",
    "\n",
    "        # Create data subsets for the current fold\n",
    "        train_subset = torch.utils.data.Subset(dataset_sparsity, train_idx)\n",
    "        val_subset = torch.utils.data.Subset(dataset_sparsity, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Initialize the model\n",
    "        model = GraphNetwork_Multi(32).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "        loss_fn = torch.nn.NLLLoss()\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            loss = train(model, loss_fn, device, train_loader, optimizer)\n",
    "            tot_correct, tot_samples = eval(model, device, val_loader)\n",
    "            accuracy, precision, recall = eval_precision(model, device, val_loader)\n",
    "            print(f\"Epoch {epoch+1}: Fold {fold+1} Validation Accuracy: {accuracy:.4f}\") \n",
    "        end_time = time.time()\n",
    "\n",
    "        fold_accuracy.append(accuracy)\n",
    "        fold_precision.append(precision)\n",
    "        fold_recall.append(recall)\n",
    "        fold_time.append(end_time - start_time)\n",
    "\n",
    "    sparsity_accuracy.append(fold_accuracy)\n",
    "    sparsity_precision.append(fold_precision)\n",
    "    sparsity_recall.append(fold_recall)\n",
    "    sparsity_time.append(fold_time)\n",
    "    \n",
    "    shutil.rmtree('multigraph_sparsity_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e9bd6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = []\n",
    "\n",
    "for i in range(train_dataset[0]['x2'].shape[1]):\n",
    "    occluded_data = test_dataset.copy()\n",
    "    for data in occluded_data:\n",
    "        data['x2'][:, i] = 0\n",
    "    \n",
    "    occluded_loader = DataLoader(occluded_data, batch_size=32, shuffle=False)\n",
    "    original_output = eval_precision(model, device, test_loader)\n",
    "    occluded_output = eval_precision(model, device, occluded_loader)\n",
    "\n",
    "    output_change = occluded_output[0] - original_output[0]\n",
    "    feature_importances.append(output_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68119a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_ROI_indices = [int(i) for i in list(np.argsort(feature_importances)[:10])]\n",
    "print(key_ROI_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191881d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a specific network to plot\n",
    "selected_network_img = image.index_img(atlas_2.maps, 68)  # Change index for network\n",
    "\n",
    "# Plotting\n",
    "plotting.plot_stat_map(selected_network_img, display_mode='ortho', cut_coords=(0, 0, 0))\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1685a9f",
   "metadata": {},
   "source": [
    "### Single Graph GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = GraphNetwork_Pheno(32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "losses = []\n",
    "\n",
    "for epoch in range(0, 200):\n",
    "    loss = train(model, loss_fn, device, train_loader, optimizer)\n",
    "    train_correct, train_samples = eval(model, device, train_loader)\n",
    "    test_correct, test_samples = eval(model, device, test_loader)\n",
    "\n",
    "    losses.append(loss)\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02d}, '\n",
    "          f'Loss: {loss:.4f}, '\n",
    "          f'Train: {100 * train_correct / train_samples:.2f}%, '\n",
    "          f'Test: {100 * test_correct / test_samples:.2f}%')\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f55990",
   "metadata": {},
   "source": [
    "## Shallow Embedding followed by SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1e499d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_node2vec(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def embed_node2vec(dataset, epochs=30, p_in=1, q_in=1):\n",
    "    # Set model\n",
    "    data = dataset\n",
    "    model = Node2Vec(data.edge_index, embedding_dim=16, walk_length=10,\n",
    "                     context_size=8, walks_per_node=10,\n",
    "                     num_negative_samples=1, p=p_in, q=q_in, sparse=True).to(device)\n",
    "    loader = model.loader(batch_size=128, shuffle=True, num_workers=4)\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "    # Train model\n",
    "    for epoch in range(1, epochs+1):\n",
    "        loss = train_node2vec(model, loader, optimizer)\n",
    "        if epoch%10 == 0:\n",
    "            print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "    \n",
    "    return model.embedding.weight.data.cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35d079a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embeddings = []\n",
    "labels = []\n",
    "\n",
    "for data_i, data in enumerate(dataset[:50]):\n",
    "    print('subject:', data_i)\n",
    "    embedding = embed_node2vec(data, epochs=200, p_in=1, q_in=1)\n",
    "    graph_embeddings.append(embedding)\n",
    "    labels.append(data.y.item())\n",
    "\n",
    "graph_embeddings = np.array(graph_embeddings)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906759db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(graph_embeddings, labels, test_size=0.2)\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "predictions = svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy of SVM on graph embeddings:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c23b0b",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4918b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert data into PyTorch tensors\n",
    "X_reshaped = np.expand_dims(corr_matrices_1, axis=1)\n",
    "X_tensor = torch.Tensor(X_reshaped) \n",
    "y_tensor = torch.Tensor(phenotypic_data_1[:,0]).long()\n",
    "\n",
    "# Split data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=10, shuffle=False)\n",
    "\n",
    "# Define the CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1) \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2) \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.feature_size = 64 * (height // 4) * (width // 4)\n",
    "        self.fc1 = nn.Linear(self.feature_size, 100)\n",
    "        self.fc2 = nn.Linear(100, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.feature_size)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Initialize the CNN\n",
    "height = 39\n",
    "width = 39\n",
    "num_classes = 2\n",
    "model = CNN(num_classes).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()  # Move to GPU if available\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1123de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
